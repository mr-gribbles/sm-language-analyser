Reddit Corpus Generation Pipeline
This project contains a modular Python pipeline for automatically sourcing text data from Reddit, processing it, and optionally transforming it using a Large Language Model (LLM). It is designed to create structured, high-quality text corpora for Natural Language Processing (NLP) tasks.

The pipeline is built with a strong emphasis on compliance, modularity, and best practices, as detailed in the accompanying technical guide.

Important Notice on Compliance and Terms of Service
This project is designed to operate in compliance with Reddit's Developer Terms. As per Section 1.2 of the technical guide, it is critical to understand the following:

PROHIBITED USE: You cannot use the data collected or generated by this pipeline (either the original posts or the rewritten versions) to train, fine-tune, or otherwise improve any AI or machine learning models. This is a direct violation of Reddit's terms.

PERMISSIBLE USE: The corpora generated by this pipeline are intended for non-training NLP tasks, such as:

Evaluating the performance of existing, pre-trained models.

Use as a knowledge base in a Retrieval-Augmented Generation (RAG) system.

Linguistic analysis and research.

Testing and developing prompt engineering strategies.

By using this software, you acknowledge and agree to adhere to these restrictions.

Features
Modular Architecture: The code is split into logical modules for configuration, Reddit scraping, data cleaning, LLM interaction, and corpus management.

Dual Pipelines: Includes two separate, independent pipelines:

A pipeline to collect and save original, untouched Reddit posts.

A pipeline to collect posts, have them rewritten by an LLM (Google Gemini), and save the original-rewritten pairs.

Configurable: Easily change target subreddits, number of posts to collect, LLM model, and output directories via a central config.py file.

Data Cleaning: Includes a standard cleaning protocol to remove HTML and normalize whitespace.

Structured Output: Saves data in the highly scalable JSONL format with comprehensive metadata for traceability.

Rate-Limit Aware: Respects API rate limits with configurable delays to ensure stable and polite data collection.

Project Structure
The project is organized into the following key files:

.
├── .env                  # (Must be created by user) For storing secret API keys
├── .gitignore            # Specifies files and folders to ignore for version control
├── config.py             # All user-configurable settings and parameters
├── requirements.txt      # A list of all Python dependencies for the project
│
├── reddit_client.py      # Handles PRAW initialization and authentication
├── reddit_scraper.py     # Contains the function for fetching posts from Reddit
├── data_cleaner.py       # Module for cleaning raw text
├── llm_rewriter.py       # Module for interacting with the Gemini API
├── corpus_manager.py     # Handles the creation and saving of corpus records
│
├── main_unedited_pipeline.py  # Main script to run the original-only collection
└── main_rewriter_pipeline.py    # Main script to run the LLM rewriting pipeline

Setup and Installation
Follow these steps to set up and run the project locally.

1. Clone the Repository

git clone [https://github.com/your-username/your-repository-name.git](https://github.com/your-username/your-repository-name.git)
cd your-repository-name

2. Set Up a Virtual Environment

It is highly recommended to use a Python virtual environment to manage dependencies.

# Create a virtual environment (e.g., named .venv)
python3 -m venv .venv

# Activate the virtual environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.\.venv\Scripts\activate

3. Install Dependencies

Install all the required Python packages using the requirements.txt file.

pip install -r requirements.txt

4. Configure Environment Variables

This is a critical step for securely managing your API keys.

Create a new file in the root of the project named .env.

Add your secret API keys to this file. This file is listed in .gitignore and will not be uploaded to GitHub.

# .env file

# Reddit API Credentials
REDDIT_CLIENT_ID="YOUR_CLIENT_ID_FROM_REDDIT"
REDDIT_CLIENT_SECRET="YOUR_CLIENT_SECRET_FROM_REDDIT"
REDDIT_USER_AGENT="A_UNIQUE_AND_DESCRIPTIVE_USER_AGENT (e.g., MyCorpusBot/1.0 by u/YourUsername)"

# Google Gemini API Key
GEMINI_API_KEY="YOUR_GEMINI_API_KEY_FROM_AI_STUDIO"

Usage
You have two main pipelines you can run. Choose one based on the type of data you wish to collect.

To Collect Original, Unedited Posts

This pipeline will fetch posts from the subreddits defined in config.py and save them directly to the corpora/original_only directory.

python main_unedited_pipeline.py

To Collect and Rewrite Posts with an LLM

This pipeline will fetch posts, send them to the Gemini API for rewriting, and save the original-rewritten pairs to the corpora/rewritten_pairs directory.

python main_rewriter_pipeline.py

Configuration
You can easily customize the behavior of the pipelines by editing the config.py file. Key settings include:

SUBREDDIT_LIST: A list of subreddits to source data from.

NUM_POSTS_TO_COLLECT: The target number of posts to collect per run.

LLM_MODEL: The specific Gemini model to use for rewriting.

SLEEP_TIMER: The delay between API calls to respect rate limits. Ensure this is set appropriately for your chosen model's RPM limit.

ORIGINAL_ONLY_DIR and REWRITTEN_PAIRS_DIR: The output directories for the two pipelines

