"""Neural network binary classifier for AI vs Human text detection.

This module implements a PyTorch neural network that can classify text as either
AI-generated or human-written using the corpus data generated by this pipeline.
The classifier uses TF-IDF vectorization for feature extraction and a deep
neural network with batch normalization and dropout for robust classification.
"""
import json
import os
import pickle
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns


class TextClassifierNetwork(nn.Module):
    """PyTorch neural network for binary text classification."""
    
    def __init__(self, input_dim: int, dropout_rate: float = 0.5):
        """Initialize the neural network architecture.
        
        Args:
            input_dim: Number of input features from TF-IDF vectorization.
            dropout_rate: Dropout rate for regularization.
        """
        super(TextClassifierNetwork, self).__init__()
        
        # Simpler architecture to reduce overfitting
        self.network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.8),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.6),
            
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the network.
        
        Args:
            x: Input tensor of shape (batch_size, input_dim).
            
        Returns:
            Output tensor of shape (batch_size, 1) with sigmoid activation.
        """
        return self.network(x)


class AIHumanTextClassifier:
    """Binary classifier to distinguish between AI-generated and human-written text."""
    
    def __init__(self, max_features: int = 10000, ngram_range: Tuple[int, int] = (1, 3)):
        """Initialize the classifier.
        
        Args:
            max_features: Maximum number of features for TF-IDF vectorization.
            ngram_range: Range of n-grams to extract (default: unigrams to trigrams).
        """
        self.max_features = max_features
        self.ngram_range = ngram_range
        self.vectorizer = None
        self.scaler = None
        self.model = None
        self.feature_names = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_corpus_data(self, corpus_dir: str) -> Tuple[List[str], List[int]]:
        """Load and prepare training data from corpus files.
        
        Args:
            corpus_dir: Directory containing the corpus files.
            
        Returns:
            Tuple of (texts, labels) where labels are 0 for human, 1 for AI.
        """
        corpus_path = Path(corpus_dir)
        texts = []
        labels = []
        
        # Load human-written texts (label = 0)
        original_dir = corpus_path / "original_only"
        if original_dir.exists():
            for file_path in original_dir.glob("*.jsonl"):
                print(f"Loading human texts from: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            record = json.loads(line.strip())
                            # Use cleaned text if available, otherwise raw text
                            if 'cleaned_text' in record['original_content']:
                                text = record['original_content']['cleaned_text']
                            elif 'cleaned_selftext' in record['original_content']:
                                text = record['original_content']['cleaned_selftext']
                            else:
                                # Fallback to raw text
                                text = record['original_content'].get('raw_text', 
                                      record['original_content'].get('raw_selftext', ''))
                            
                            if text and len(text.strip()) > 10:  # Filter very short texts
                                texts.append(text.strip())
                                labels.append(0)  # Human-written
                        except (json.JSONDecodeError, KeyError) as e:
                            print(f"Error processing line in {file_path}: {e}")
                            continue
        
        # Load AI-rewritten texts (label = 1)
        rewritten_dir = corpus_path / "rewritten_pairs"
        if rewritten_dir.exists():
            for file_path in rewritten_dir.glob("*.jsonl"):
                print(f"Loading AI texts from: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            record = json.loads(line.strip())
                            if record.get('llm_transformation') and record['llm_transformation'].get('rewritten_text'):
                                text = record['llm_transformation']['rewritten_text']
                                if text and len(text.strip()) > 10:  # Filter very short texts
                                    texts.append(text.strip())
                                    labels.append(1)  # AI-generated
                        except (json.JSONDecodeError, KeyError) as e:
                            print(f"Error processing line in {file_path}: {e}")
                            continue
        
        print(f"Loaded {len(texts)} texts total:")
        print(f"  Human texts: {labels.count(0)}")
        print(f"  AI texts: {labels.count(1)}")
        
        return texts, labels
    
    def extract_features(self, texts: List[str]) -> np.ndarray:
        """Extract TF-IDF features from texts.
        
        Args:
            texts: List of text strings.
            
        Returns:
            Feature matrix as numpy array.
        """
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                max_features=self.max_features,
                ngram_range=self.ngram_range,
                stop_words='english',
                lowercase=True,
                strip_accents='unicode',
                token_pattern=r'\b[a-zA-Z]{2,}\b',  # Only alphabetic tokens, min 2 chars
                min_df=2,  # Ignore terms that appear in less than 2 documents
                max_df=0.95,  # Ignore terms that appear in more than 95% of documents
                sublinear_tf=True,  # Apply sublinear tf scaling
                use_idf=True,  # Enable inverse-document-frequency reweighting
                smooth_idf=True,  # Smooth idf weights
                norm='l2'  # L2 normalization
            )
            features = self.vectorizer.fit_transform(texts)
            self.feature_names = self.vectorizer.get_feature_names_out()
        else:
            features = self.vectorizer.transform(texts)
        
        return features.toarray()
    
    def _create_data_loader(self, features: np.ndarray, labels: np.ndarray, 
                           batch_size: int, shuffle: bool = True) -> DataLoader:
        """Create a PyTorch DataLoader from features and labels.
        
        Args:
            features: Feature matrix.
            labels: Target labels.
            batch_size: Batch size for the DataLoader.
            shuffle: Whether to shuffle the data.
            
        Returns:
            PyTorch DataLoader.
        """
        features_tensor = torch.FloatTensor(features)
        labels_tensor = torch.FloatTensor(labels).unsqueeze(1)
        dataset = TensorDataset(features_tensor, labels_tensor)
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    
    def load_corpus_files(self, human_file: str, ai_file: str) -> Tuple[List[str], List[int]]:
        """Load and prepare training data from specific corpus files.
        
        Args:
            human_file: Path to JSONL file containing human-written texts.
            ai_file: Path to JSONL file containing AI-generated texts.
            
        Returns:
            Tuple of (texts, labels) where labels are 0 for human, 1 for AI.
        """
        texts = []
        labels = []
        
        # Load human-written texts (label = 0)
        print(f"Loading human texts from: {human_file}")
        with open(human_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    # Use cleaned text if available, otherwise raw text
                    if 'cleaned_text' in record['original_content']:
                        text = record['original_content']['cleaned_text']
                    elif 'cleaned_selftext' in record['original_content']:
                        text = record['original_content']['cleaned_selftext']
                    else:
                        # Fallback to raw text
                        text = record['original_content'].get('raw_text', 
                              record['original_content'].get('raw_selftext', ''))
                    
                    if text and len(text.strip()) > 10:  # Filter very short texts
                        texts.append(text.strip())
                        labels.append(0)  # Human-written
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Error processing line in {human_file}: {e}")
                    continue
        
        # Load AI-rewritten texts (label = 1)
        print(f"Loading AI texts from: {ai_file}")
        with open(ai_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    if record.get('llm_transformation') and record['llm_transformation'].get('rewritten_text'):
                        text = record['llm_transformation']['rewritten_text']
                        if text and len(text.strip()) > 10:  # Filter very short texts
                            texts.append(text.strip())
                            labels.append(1)  # AI-generated
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Error processing line in {ai_file}: {e}")
                    continue
        
        print(f"Loaded {len(texts)} texts total:")
        print(f"  Human texts: {labels.count(0)}")
        print(f"  AI texts: {labels.count(1)}")
        
        return texts, labels
    
    def _train_model(self, texts: List[str], labels: List[int], test_size: float = 0.2, 
                    validation_size: float = 0.15, epochs: int = 100, batch_size: int = 32, 
                    learning_rate: float = 0.0005, patience: int = 20) -> Dict[str, Any]:
        """Internal method to train the model on prepared data.
        
        Args:
            texts: List of text strings.
            labels: List of corresponding labels (0=human, 1=AI).
            test_size: Proportion of data to use for testing.
            validation_size: Proportion of training data to use for validation.
            epochs: Maximum number of training epochs.
            batch_size: Training batch size.
            learning_rate: Learning rate for the optimizer.
            patience: Early stopping patience.
            
        Returns:
            Training history and evaluation metrics.
        """
        if len(texts) == 0:
            raise ValueError("No texts provided for training")
        
        # Extract features
        print("Extracting TF-IDF features...")
        features = self.extract_features(texts)
        
        # Scale features
        self.scaler = StandardScaler()
        features_scaled = self.scaler.fit_transform(features)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            features_scaled, labels, test_size=test_size, random_state=42, stratify=labels
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=validation_size, random_state=42, stratify=y_train
        )
        
        print(f"Training set: {len(X_train)} samples")
        print(f"Validation set: {len(X_val)} samples")
        print(f"Test set: {len(X_test)} samples")
        
        # Create data loaders
        train_loader = self._create_data_loader(X_train, y_train, batch_size, shuffle=True)
        val_loader = self._create_data_loader(X_val, y_val, batch_size, shuffle=False)
        
        # Initialize model
        self.model = TextClassifierNetwork(features_scaled.shape[1]).to(self.device)
        criterion = nn.BCELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, 
                                                        patience=5, min_lr=1e-6)
        
        print(f"Model built with {features_scaled.shape[1]} input features")
        print(f"Using device: {self.device}")
        
        # Training loop
        history = {
            'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []
        }
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print("Training model...")
        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_features, batch_labels in train_loader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_features)
                loss = criterion(outputs, batch_labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                predicted = (outputs > 0.5).float()
                train_total += batch_labels.size(0)
                train_correct += (predicted == batch_labels).sum().item()
            
            # Validation phase
            self.model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_features, batch_labels in val_loader:
                    batch_features = batch_features.to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    outputs = self.model(batch_features)
                    loss = criterion(outputs, batch_labels)
                    
                    val_loss += loss.item()
                    predicted = (outputs > 0.5).float()
                    val_total += batch_labels.size(0)
                    val_correct += (predicted == batch_labels).sum().item()
            
            # Calculate metrics
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            # Update history
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(train_acc)
            history['val_acc'].append(val_acc)
            
            # Learning rate scheduling
            scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model state
                best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] - "
                      f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        # Load best model
        self.model.load_state_dict(best_model_state)
        
        # Evaluate on test set
        print("Evaluating on test set...")
        test_loader = self._create_data_loader(X_test, y_test, batch_size, shuffle=False)
        
        self.model.eval()
        test_predictions = []
        test_probabilities = []
        
        with torch.no_grad():
            for batch_features, _ in test_loader:
                batch_features = batch_features.to(self.device)
                outputs = self.model(batch_features)
                probabilities = outputs.cpu().numpy().flatten()
                predictions = (outputs > 0.5).float().cpu().numpy().flatten()
                
                test_predictions.extend(predictions)
                test_probabilities.extend(probabilities)
        
        test_predictions = np.array(test_predictions, dtype=int)
        test_probabilities = np.array(test_probabilities)
        
        # Calculate metrics
        test_accuracy = accuracy_score(y_test, test_predictions)
        class_report = classification_report(y_test, test_predictions, 
                                           target_names=['Human', 'AI'], output_dict=True)
        
        results = {
            'history': history,
            'test_accuracy': test_accuracy,
            'test_precision': class_report['weighted avg']['precision'],
            'test_recall': class_report['weighted avg']['recall'],
            'classification_report': class_report,
            'confusion_matrix': confusion_matrix(y_test, test_predictions),
            'feature_count': features_scaled.shape[1]
        }
        
        print(f"\nTest Results:")
        print(f"Accuracy: {test_accuracy:.4f}")
        print(f"Precision: {results['test_precision']:.4f}")
        print(f"Recall: {results['test_recall']:.4f}")
        print(f"\nClassification Report:")
        print(classification_report(y_test, test_predictions, target_names=['Human', 'AI']))
        
        return results
    
    def train(self, corpus_dir: str, test_size: float = 0.2, validation_size: float = 0.1, 
              epochs: int = 100, batch_size: int = 32, learning_rate: float = 0.001,
              patience: int = 10) -> Dict[str, Any]:
        """Train the classifier on corpus data from directory structure.
        
        Args:
            corpus_dir: Directory containing corpus files.
            test_size: Proportion of data to use for testing.
            validation_size: Proportion of training data to use for validation.
            epochs: Maximum number of training epochs.
            batch_size: Training batch size.
            learning_rate: Learning rate for the optimizer.
            patience: Early stopping patience.
            
        Returns:
            Training history and evaluation metrics.
        """
        # Load data from directory structure
        texts, labels = self.load_corpus_data(corpus_dir)
        
        # Train using the internal method
        return self._train_model(texts, labels, test_size, validation_size, epochs, 
                               batch_size, learning_rate, patience)
    
    def train_from_files(self, human_file: str, ai_file: str, test_size: float = 0.2, 
                        validation_size: float = 0.1, epochs: int = 100, batch_size: int = 32, 
                        learning_rate: float = 0.001, patience: int = 10) -> Dict[str, Any]:
        """Train the classifier on corpus data from specific files.
        
        Args:
            human_file: Path to JSONL file containing human-written texts.
            ai_file: Path to JSONL file containing AI-generated texts.
            test_size: Proportion of data to use for testing.
            validation_size: Proportion of training data to use for validation.
            epochs: Maximum number of training epochs.
            batch_size: Training batch size.
            learning_rate: Learning rate for the optimizer.
            patience: Early stopping patience.
            
        Returns:
            Training history and evaluation metrics.
        """
        # Load data from specific files
        texts, labels = self.load_corpus_files(human_file, ai_file)
        
        # Train using the internal method
        return self._train_model(texts, labels, test_size, validation_size, epochs, 
                               batch_size, learning_rate, patience)
    
    def predict(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """Predict whether texts are AI-generated or human-written.
        
        Args:
            texts: List of text strings to classify.
            
        Returns:
            Tuple of (predictions, probabilities) where predictions are 0/1 and
            probabilities are confidence scores for AI class.
        """
        if self.model is None or self.vectorizer is None or self.scaler is None:
            raise ValueError("Model not trained. Call train() first.")
        
        # Extract and scale features
        features = self.extract_features(texts)
        features_scaled = self.scaler.transform(features)
        
        # Convert to tensor and predict
        features_tensor = torch.FloatTensor(features_scaled).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(features_tensor)
            probabilities = outputs.cpu().numpy().flatten()
            predictions = (outputs > 0.5).float().cpu().numpy().flatten().astype(int)
        
        return predictions, probabilities
    
    def save_model(self, model_path: str):
        """Save the trained model and preprocessing components.
        
        Args:
            model_path: Path to save the model (without extension).
        """
        if self.model is None:
            raise ValueError("No model to save. Train the model first.")
        
        model_path = Path(model_path)
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Save PyTorch model
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_dim': self.model.network[0].in_features,
                'max_features': self.max_features,
                'ngram_range': self.ngram_range
            }
        }, f"{model_path}_model.pth")
        
        # Save preprocessing components
        with open(f"{model_path}_vectorizer.pkl", 'wb') as f:
            pickle.dump(self.vectorizer, f)
        
        with open(f"{model_path}_scaler.pkl", 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print(f"Model saved to {model_path}_model.pth")
        print(f"Vectorizer saved to {model_path}_vectorizer.pkl")
        print(f"Scaler saved to {model_path}_scaler.pkl")
    
    def load_model(self, model_path: str):
        """Load a trained model and preprocessing components.
        
        Args:
            model_path: Path to the saved model (without extension).
        """
        model_path = Path(model_path)
        
        # Load model
        checkpoint = torch.load(f"{model_path}_model.pth", map_location=self.device)
        model_config = checkpoint['model_config']
        
        # Update instance variables from saved config
        self.max_features = model_config['max_features']
        self.ngram_range = model_config['ngram_range']
        
        # Initialize and load model
        self.model = TextClassifierNetwork(model_config['input_dim']).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # Load preprocessing components
        with open(f"{model_path}_vectorizer.pkl", 'rb') as f:
            self.vectorizer = pickle.load(f)
        
        with open(f"{model_path}_scaler.pkl", 'rb') as f:
            self.scaler = pickle.load(f)
        
        print(f"Model loaded from {model_path}")
    
    def plot_training_history(self, history: Dict[str, List[float]], save_path: Optional[str] = None):
        """Plot training history.
        
        Args:
            history: Training history from model training.
            save_path: Optional path to save the plot.
        """
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss plot
        axes[0].plot(history['train_loss'], label='Training Loss')
        axes[0].plot(history['val_loss'], label='Validation Loss')
        axes[0].set_title('Model Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        
        # Accuracy plot
        axes[1].plot(history['train_acc'], label='Training Accuracy')
        axes[1].plot(history['val_acc'], label='Validation Accuracy')
        axes[1].set_title('Model Accuracy')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy')
        axes[1].legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Training history plot saved to {save_path}")
        
        plt.show()
    
    def plot_confusion_matrix(self, confusion_matrix: np.ndarray, save_path: Optional[str] = None):
        """Plot confusion matrix.
        
        Args:
            confusion_matrix: Confusion matrix from sklearn.
            save_path: Optional path to save the plot.
        """
        plt.figure(figsize=(8, 6))
        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'])
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Confusion matrix plot saved to {save_path}")
        
        plt.show()


def main():
    """Example usage of the classifier."""
    # Initialize classifier
    classifier = AIHumanTextClassifier(max_features=15000, ngram_range=(1, 3))
    
    # Train the model
    corpus_dir = "corpora"
    results = classifier.train(corpus_dir, epochs=50, batch_size=64)
    
    # Plot results
    classifier.plot_training_history(results['history'], 'training_history.png')
    classifier.plot_confusion_matrix(results['confusion_matrix'], 'confusion_matrix.png')
    
    # Save the model
    classifier.save_model("models/ai_human_classifier")
    
    # Example predictions
    test_texts = [
        "This is a test sentence to see if the model works correctly.",
        "The implementation demonstrates sophisticated natural language processing capabilities.",
        "hey whats up everyone hope you're having a good day lol"
    ]
    
    predictions, probabilities = classifier.predict(test_texts)
    
    for i, text in enumerate(test_texts):
        label = "AI" if predictions[i] == 1 else "Human"
        confidence = probabilities[i] if predictions[i] == 1 else 1 - probabilities[i]
        print(f"Text: {text[:50]}...")
        print(f"Prediction: {label} (confidence: {confidence:.3f})")
        print()


if __name__ == "__main__":
    main()
